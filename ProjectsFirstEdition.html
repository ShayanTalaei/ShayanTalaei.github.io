<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Shayan Talaei</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
        <script src="https://code.jquery.com/jquery-1.10.2.js"></script>
    </head>
    <body id="page-top">
        
        <!--Navigation bar-->
<div id="nav-placeholder">

</div>

<script>
$(function(){
  $("#nav-placeholder").load("navbar.html");
});
</script>
<!--end of Navigation bar-->
        
        <!-- Page Content-->
        <div class="container-fluid p-0">
            
            

            
            <!--research internship-->
            <section class="resume-section" id="internships">
                <div class="resume-section-content">
                    <h2 class="mb-5">Research Internships and Projects</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Internship on Machine Learning Theory </h3>
                            
                            <div class="subheading mb-3">
                            <p>Swiss Federal Institute of Technology in Lausanne (EPFL Switzerland)</p>
                            </div>
                            <p>Supervised by: <a href="https://sma.epfl.ch/~abbe">Prof. Emmanuel Abbe</a>,<a href="https://misiakie.github.io/"> Theodor Misiakiewicz</a>
                            </p>
                            <li><strong>Evolving Kernels</strong>
                            </li>
                            <p>We developed an algorithm inspired by <a href="https://proceedings.mlr.press/v178/abbe22a/abbe22a.pdf">the merged-staircase property</a> to combine kernels iteratively. We constructed
a sequence of kernels, such that each kernel is derived from previous kernels and their corresponding models, so that
it outperformed all the previous models. Using this technique, kernel methods can be improved to compete with
neural networks on real-world datasets, specifically in small-size sample regimes.
                            </p>
                            <li><strong>The Effect of Bottlenecks on Reasoning Neural Networks</strong>
                            </li>
                            <p>We studied the effect of binary bottlenecks in the neural network architectures. We observed that, using sufficiently
large dataset and the correct-size bottleneck, the model will be forced to do reasoning instead of memorizing the
inputs.
                            </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">June 2022 - Present</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Internship on Distributed and Federated Optimization</h3>
                            
                            <div class="subheading mb-3">
                            <p>Institute of Science and Technology Austria (IST Austria)</p>
                            </div>
                            <p>Supervised by: <a href="http://people.csail.mit.edu/alistarh/">Prof. Dan Alistarh</a>, <a href="https://scholar.google.com/citations?hl=en&user=iq-7vhMAAAAJ&view_op=list_works">Dr. Giorgi Nadiradze</a>
                            </p>
                            <li><strong>Quantized Asynchronous Federated Learning</strong>
                            </li>
                            <p>We presented a new variant of the classic federated averaging (FedAvg), which supports both asynchronous
communication and communication compression. I implemented a time-based simulator using Pytorch to study the
behavior of different federated learning algorithms.
                            </p>
                            <li><strong>Hybrid Decentralized Optimization</strong>
                            </li>
                            <p>We initiated the study of ”Hybrid Decentralised Optimization”, where nodes with zeroth-order and first-order
capabilities jointly attempt to solve a distributed optimization task. To prove a linear speedup in convergence rate,
I proposed a new analysis technique that works even with noisy gradient estimators. Moreover, I implemented
a distributed simulator to study the convergence behaviour of our algorithm combined with different gradient
estimators.

                            </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">July 2021 - June 2022</span></div>
                    </div>              
                </div>
            <hr class="m-0" />
            </section>
            
            
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
